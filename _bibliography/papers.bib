---
---

@article{tachella2022sampling,
  title={Sampling Theorems for Learning from Incomplete Measurements},
  author={Tachella, Julian and Chen, Dongdong and Davies, Mike},
  journal={arXiv preprint},
  arxiv={2201.12151},
  year={2022},
  month = {January},
  abbr = {tachella2022sampling.PNG},
  project = {equivariantimaging},
  abstract = {In many real-world settings, only incomplete measurement data are available which can pose a problem for learning. Unsupervised learning of the signal model using a fixed incomplete measurement process is impossible in general, as there is no information in the nullspace of the measurement operator. This limitation can be overcome by using measurements from multiple operators. While this idea has been successfully applied in various applications, a precise characterization of the conditions for learning is still lacking. In this paper, we fill this gap by presenting necessary and sufficient conditions for learning the signal model which indicate the interplay between the number of distinct measurement operators G, the number of measurements per operator m, the dimension of the model k and the dimension of the signals n. In particular, we show that generically unsupervised learning is possible if each operator obtains at least m>k+n/G measurements. Our results are agnostic of the learning algorithm and have implications in a wide range of practical algorithms, from low-rank matrix recovery to deep neural networks.},
}

@INPROCEEDINGS{sheehan2021detection,
  author={Sheehan, Michael P. and Tachella, Julian and Davies, Mike E.},
  booktitle={2021 29th European Signal Processing Conference (EUSIPCO)}, 
  title={Surface Detection for Sketched Single Photon Lidar}, 
  year={2021},
  month = {August},
  volume={},
  number={},
  arxiv = {2105.06920},
  code = {https://gitlab.com/Tachella/sketched_lidar},
  pages={621-625},
  abstract = {Single-photon lidar devices are able to collect an ever-increasing amount of time-stamped photons in small time periods due to increasingly larger arrays, generating a memory and computational bottleneck on the data processing side. Recently, a sketching technique was introduced to overcome this bottleneck which compresses the amount of information to be stored and processed. The size of the sketch scales with the number of underlying parameters of the time delay distribution and not, fundamentally, with either the number of detected photons or the time-stamp resolution. In this paper, we propose a detection algorithm based solely on a small sketch that determines if there are surfaces or objects in the scene or not. If a surface is detected, the depth and intensity of a single object can be computed in closed-form directly from the sketch. The computational load of the proposed detection algorithm depends solely on the size of the sketch, in contrast to previous algorithms that depend at least linearly in the number of collected photons or histogram bins, paving the way for fast, accurate and memory efficient lidar estimation. Our experiments demonstrate the memory and statistical efficiency of the proposed algorithm both on synthetic and real lidar datasets.},
  doi={10.23919/EUSIPCO54536.2021.9616208},
  abbr = {sheehan2021detection.png},
  }

@article{chen2021robust,
  title={Robust Equivariant Imaging: a fully unsupervised framework for learning to image from noisy and partial measurements},
  author={Chen, Dongdong and Tachella, Julian and Davies, Mike E},
  journal={Accepted at CVPR'22},
  year={2021},
  arxiv={2111.12855},
  month = {December},
  abbr = {chen2021robust.PNG},
  project = {equivariantimaging},
  code = {https://github.com/edongdongchen/REI},
  abstract = {Deep networks provide state-of-the-art performance in multiple imaging inverse problems ranging from medical imaging to computational photography. However, most existing networks are trained with clean signals which are often hard or impossible to obtain. Equivariant imaging (EI) is a recent self-supervised learning framework that exploits the group invariance present in signal distributions to learn a reconstruction function from partial measurement data alone. While EI results are impressive, its performance degrades with increasing noise. In this paper, we propose a Robust Equivariant Imaging (REI) framework which can learn to image from noisy partial measurements alone. The proposed method uses Stein's Unbiased Risk Estimator (SURE) to obtain a fully unsupervised training loss that is robust to noise. We show that REI leads to considerable performance gains on linear and nonlinear inverse problems, thereby paving the way for robust unsupervised imaging with deep networks.},
}

@inproceedings{chen2021equivariant,
  title={Equivariant Imaging: Learning Beyond the Range Space},
  author={Chen, Dongdong and Tachella, Julian and Davies, Mike E},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages={4379--4388},
  year={2021},
  pdf ={https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf},
  sup ={https://openaccess.thecvf.com/content/ICCV2021/supplemental/Chen_Equivariant_Imaging_Learning_ICCV_2021_supplemental.zip}, 
  month = {March},
  abbr = {chen2021equivariant.jpg},
  code = {https://github.com/edongdongchen/EI},
  selected={true},
  project = {equivariantimaging},
  abstract = {In various imaging problems, we only have access to compressed measurements of the underlying signals, hindering most learning-based strategies which usually require pairs of signals and associated measurements for training. Learning only from compressed measurements is impossible in general, as the compressed observations do not contain information outside the range of the forward sensing operator. We propose a new end-to-end self-supervised framework that overcomes this limitation by exploiting the equivariances present in natural signals. Our proposed learning strategy performs as well as fully supervised methods. Experiments demonstrate the potential of this framework on inverse problems including sparse-view X-ray computed tomography on real clinical data and image inpainting on natural images.},
}

@article{sheehan2021sketching,
  title={A sketching framework for reduced data transfer in photon counting lidar},
  author={Sheehan, Michael P and Tachella, Julian and Davies, Mike E},
  journal={IEEE Transactions on Computational Imaging},
  volume={7},
  pages={989--1004},
  year={2021},
  arxiv = {2102.08732},
  abbr = {sheehan2021sketching.gif},
  pdf = {https://ieeexplore.ieee.org/abstract/document/9541047},
  code = {https://gitlab.com/Tachella/sketched_lidar},
  abstract = {Single-photon lidar has become a prominent tool for depth imaging in recent years. At the core of the technique, the depth of a target is measured by constructing a histogram of time delays between emitted light pulses and detected photon arrivals. A major data processing bottleneck arises on the device when either the number of photons per pixel is large or the resolution of the time-stamp is fine, as both the space requirement and the complexity of the image reconstruction algorithms scale with these parameters. We solve this limiting bottleneck of existing lidar techniques by sampling the characteristic function of the time of flight (ToF) model to build a compressive statistic, a so-called sketch of the time delay distribution, which is sufficient to infer the spatial distance and intensity of the object. The size of the sketch scales with the degrees of freedom of the ToF model (number of objects) and not, fundamentally, with the number of photons or the time-stamp resolution. Moreover, the sketch is highly amenable for on-chip online processing. We show theoretically that the loss of information for compression is controlled and the mean squared error of the inference quickly converges towards the optimal Cramér-Rao bound (i.e. no loss of information) for modest sketch sizes. The proposed compressed single-photon lidar framework is tested and evaluated on real life datasets of complex scenes where it is shown that a compression rate of up-to 150 is achievable in practice without sacrificing the overall resolution of the reconstructed image.},
}

@inproceedings{tachella2021nonlocal,
    author    = {Tachella, Julian and Tang, Junqi and Davies, Mike},
    title     = {The Neural Tangent Link Between CNN Denoisers and Non-Local Filters},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8618-8627},
	abbr = {tachella2021nonlocal.png},
    project = {understandingcnns},
	code = {https://gitlab.com/Tachella/neural_tangent_denoiser},
    pdf = {https://openaccess.thecvf.com/content/CVPR2021/papers/Tachella_The_Neural_Tangent_Link_Between_CNN_Denoisers_and_Non-Local_Filters_CVPR_2021_paper.pdf},
    sup = {https://openaccess.thecvf.com/content/CVPR2021/supplemental/Tachella_The_Neural_Tangent_CVPR_2021_supplemental.pdf},
	selected={true},
	blog = {2020/ntd},
	video = {https://youtu.be/vLxzxp2boyY},
	abstract = {Convolutional Neural Networks (CNNs) are now a well-established tool for solving computational imaging problems. Modern CNN-based algorithms obtain state-of-the-art performance in diverse image restoration problems. Furthermore, it has been recently shown that, despite being highly overparameterized, networks trained with a single corrupted image can still perform as well as fully trained networks. We introduce a formal link between such networks through their neural tangent kernel (NTK), and well-known non-local filtering techniques, such as non-local means or BM3D. The filtering function associated with a given network architecture can be obtained in closed form without need to train the network, being fully characterized by the random initialization of the network weights. While the NTK theory accurately predicts the filter associated with networks trained using standard gradient descent, our analysis shows that it falls short to explain the behaviour of networks trained using the popular Adam optimizer. The latter achieves a larger change of weights in hidden layers, adapting the non-local filtering function during training. We evaluate our findings via extensive image denoising experiments.},
}


@article{rapp2020seeing,
  title={Seeing around corners with edge-resolved transient imaging},
  author={Rapp, Joshua and Saunders, Charles and Tachella, Julian and Murray-Bruce, John and Altmann, Yoann and Tourneret, Jean-Yves and McLaughlin, Stephen and Dawson, Robin and Wong, Franco NC and Goyal, Vivek K},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--10},
  year={2020},
  publisher={Nature Publishing Group},
  abbr = {rapp2020seeing.gif},
  project = {3Dreconstruction},
  abstract = {Non-line-of-sight (NLOS) imaging is a rapidly growing field seeking to form images of objects outside the field of view, with potential applications in autonomous navigation, reconnaissance, and even medical imaging. The critical challenge of NLOS imaging is that diffuse reflections scatter light in all directions, resulting in weak signals and a loss of directional information. To address this problem, we propose a method for seeing around corners that derives angular resolution from vertical edges and longitudinal resolution from the temporal response to a pulsed light source. We introduce an acquisition strategy, scene response model, and reconstruction algorithm that enable the formation of 2.5-dimensional representations—a plan view plus heights—and a 180∘ field of view for large-scale scenes. Our experiments demonstrate accurate reconstructions of hidden rooms up to 3 meters in each dimension despite a small scan aperture (1.5-centimeter radius) and only 45 measurement locations.},
  code = {https://github.com/tachella/ERTI},
  pdf = {https://www.nature.com/articles/s41467-020-19727-4.pdf},
  video = {https://youtu.be/1MDwFVky-wg},
}


@inproceedings{tachella2019crt3d,
	author={{Tachella}, Julian and {Altmann}, Yoann and {McLaughlin}, Stephen and {Tourneret}, Jean-Yves},
	booktitle={Proc. 8th Int. Workshop Comput. Adv. Multi-Sensor Adap. Process. (CAMSAP)}, 
	title={Real-time 3{D} color imaging with single-photon lidar data}, 
	year={2019}, 
	volume={}, 
	number={}, 
	address = {Guadaloupe, West Indies},
	pages={1-5}, 
	doi={10.1109/CAMSAP.2017.8313128}, 
	abbr = {tachella2019crt3d.gif},
    project = {3Dreconstruction},
	abstract = {Single-photon lidar devices can acquire 3D data at very long range with high precision. Moreover, recent advances in lidar arrays have enabled acquisitions at very high frame rates. However, these devices place a severe bottleneck on the reconstruction algorithms, which have to handle very large volumes of noisy data. Recently, real-time 3D reconstruction of distributed surfaces has been demonstrated obtaining information at one wavelength. Here, we propose a new algorithm that achieves color 3D reconstruction without increasing the execution time nor the acquisition process of the realtime single-wavelength reconstruction system. The algorithm uses a coded aperture that compresses the data by considering a subset of the wavelengths per pixel. The reconstruction algorithm is based on a plug-and-play denoising framework, which benefits from off-the-shelf point cloud and image de-noisers. Experiments using real lidar data show the competitivity of the proposed method.},
	code = {https://gitlab.com/Tachella/real-time-single-photon-lidar},
	month={Dec},}

@article{rapp2020advances,
  title={Advances in single-photon lidar for autonomous vehicles: Working principles, challenges, and recent advances},
  author={Rapp, Joshua and Tachella, Julian and Altmann, Yoann and McLaughlin, Stephen and Goyal, Vivek K},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={4},
  pages={62--71},
  year={2020},
  month = {June},
  abbr = {rapp2020advances.png},
  project = {3Dreconstruction},
  abstract = {The safety and success of autonomous vehicles (AVs) depend on their ability to accurately map and respond to their surroundings in real time. One of the most promising recent technologies for depth mapping is single-photon lidar (SPL), which measures the time of flight of individual photons. The long-range capabilities (kilometers), excellent depth resolution (centimeters), and use of low-power (eye-safe) laser sources renders this modality a strong candidate for use in AVs. While presenting unique opportunities, the remarkable sensitivity of single-photon detectors introduces several signal processing challenges. The discrete nature of photon counting and the particular design of the detection devices means the acquired signals cannot be treated as arising in a linear system with additive Gaussian noise. Moreover, the number of useful photon detections may be small despite a large data volume, thus requiring careful modeling and algorithmic design for real-time performance. This article discusses the main working principles of SPL and summarizes recent advances in signal processing techniques for this modality, highlighting promising applications in AVs as well as a number of challenges for vehicular lidar that cannot be solved by better hardware alone.},
  pdf = {https://ieeexplore.ieee.org/document/9127841},
  publisher={IEEE},
}

@article{tachella2019musapop,
	author = {{Tachella}, Julian and {Altmann}, Yoann and {M{\'a}rquez}, Miguel and
	{Arguello-Fuentes}, Henry and {Tourneret}, Jean-Yves and
	{McLaughlin}, Stephen},
	title = {Bayesian 3{D} Reconstruction of Subsampled Multispectral Single-photon Lidar Signals},
	journal = {IEEE Trans. Comput. Imag.},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
	year = {2019},
	abbr = {tachella2019musapop.gif},
	code = {https://gitlab.com/Tachella/musapop},
    project = {3Dreconstruction},
	pdf = {https://ieeexplore.ieee.org/document/8854866},
	abstract = {Light detection and ranging (Lidar) single-photon devices capture range and intensity information from a three-dimensional (3-D) scene. This modality enables long range 3-D reconstruction with high range precision and low laser power. A multispectral single-photon Lidar system provides additional spectral diversity, allowing the discrimination of different materials. However, the main drawback of such systems can be the long acquisition time needed to collect enough photons in each spectral band. In this work, we tackle this problem in two ways: first, we propose a Bayesian 3-D reconstruction algorithm that is able to find multiple surfaces per pixel, using few photons, i.e., shorter acquisitions. In contrast to previous algorithms, the novel method processes jointly all the spectral bands, obtaining better reconstructions using less photon detections. The proposed model promotes spatial correlation between neighbouring points within a given surface using spatial point processes. Secondly, we account for different spatial and spectral subsampling schemes, which reduce the total number of measurements, without significant degradation of the reconstruction performance. In this way, the total acquisition time, memory requirements and computational time can be significantly reduced. The experiments performed using both synthetic and real single-photon Lidar data demonstrate the advantages of tailored sampling schemes over random alternatives. Furthermore, the proposed algorithm yields better estimates than other existing methods for multi-surface reconstruction using multispectral Lidar data.},
	month = {Apr},
}

@article{tachella2019manipop,
	author = {Tachella, Julian and Altmann, Yoann and Ren, Ximing and McCarthy, Angus and Buller, Gerald and McLaughlin, Steve and Tourneret, Jean-Yves},
	title = {Bayesian 3{D} Reconstruction of Complex Scenes from Single-Photon Lidar Data},
	journal = {SIAM Journal on Imaging Sciences},
	volume = {12},
	number = {1},
	pages = {521-550},
	year = {2019},
	abbr = {tachella2019manipop.gif},
	doi = {10.1137/18M1183972},
	arxiv = {1810.11633},
    project = {3Dreconstruction},
	abstract = {Light detection and ranging (Lidar) data can be used to capture the depth and intensity profile of a 3D scene. This modality relies on constructing, for each pixel, a histogram of time delays between emitted light pulses and detected photon arrivals. In a general setting, more than one surface can be observed in a single pixel. The problem of estimating the number of surfaces, their reflectivity and position becomes very challenging in the low-photon regime (which equates to short acquisition times) or relatively high background levels (i.e., strong ambient illumination). This paper presents a new approach to 3D reconstruction using single-photon, single-wavelength Lidar data, which is capable of identifying multiple surfaces in each pixel. Adopting a Bayesian approach, the 3D structure to be recovered is modelled as a marked point process and reversible jump Markov chain Monte Carlo (RJ-MCMC) moves are proposed to sample the posterior distribution of interest. In order to promote spatial correlation between points belonging to the same surface, we propose a prior that combines an area interaction process and a Strauss process. New RJ-MCMC dilation and erosion updates are presented to achieve an efficient exploration of the configuration space. To further reduce the computational load, we adopt a multiresolution approach, processing the data from a coarse to the finest scale. The experiments performed with synthetic and real data show that the algorithm obtains better reconstructions than other recently published optimization algorithms for lower execution times.},
	pdf = {https://hal.archives-ouvertes.fr/hal-02185077/document},
	code = {https://gitlab.com/Tachella/manipop},
	video = {https://youtu.be/pk0tLCCqnVk},
}


@article{tachella2019rt3d,
	author = {Tachella, Julian and Altmann, Yoann and Mellado, Nicolas and Tobin, Rachel and  McCarthy, Angus and Buller, Gerald and Tourneret, Jean-Yves and McLaughlin, Steve},
	title = {{Real-time 3D reconstruction from single-photon lidar data using plug-and-play point cloud denoisers}},
	journal = {Nature Communications},
	number = {10},
	pages = {4984},
	year = {2019},
	doi = {10.1137/18M1183972},
	abstract = {Single-photon lidar has emerged as a prime candidate technology for depth imaging through challenging environments. Until now, a major limitation has been the significant amount of time required for the analysis of the recorded data. Here we show a new computational framework for real-time three-dimensional (3D) scene reconstruction from single-photon data. By combining statistical models with highly scalable computational tools from the computer graphics community, we demonstrate 3D reconstruction of complex outdoor scenes with processing times of the order of 20 ms, where the lidar data was acquired in broad daylight from distances up to 320 metres. The proposed method can handle an unknown number of surfaces in each pixel, allowing for target detection and imaging through cluttered scenes. This enables robust, real-time target reconstruction of complex moving scenes, paving the way for single-photon lidar at video rates for practical 3D imaging applications.},
	abbr = {tachella2019rt3d.gif},
	code = {https://gitlab.com/Tachella/real-time-single-photon-lidar},
	pdf = {https://www.nature.com/articles/s41467-019-12943-7.pdf},
 	selected={true},
  project = {3Dreconstruction},
	video = {https://youtu.be/PzCcAoypUfM},
}


@inproceedings{tachella2019genmanipop,
	author={{Tachella}, Julian and {Altmann}, Yoann and {McLaughlin}, Stephen and {Tourneret}, Jean-Yves},
	booktitle={Proc. Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)},
	title={3D Reconstruction Using Single-photon Lidar Data Exploiting the Widths of the Returns},
	year={2019},
	volume={},
	number={},
	pages={7815-7819},
	ISSN={1520-6149},
	code = {https://gitlab.com/Tachella/generalized-manipop},
	month={May},
  project = {3Dreconstruction},
	abbr = {tachellagenmanipop.gif},
}

@inproceedings{tachella2019detection1,
	author={{Tachella}, Julian and {Altmann}, Yoann and {McLaughlin}, Stephen and {Tourneret}, Jean-Yves},
	booktitle={Proc. 27th Eur. Signal Process. Conf. (EUSIPCO)},
	title={Fast Surface Detection in Single-Photon Lidar Waveforms},
	year={2019},
	volume={},
	number={},
	pages={1-5},
	ISSN={2219-5491},
	abbr = {tachella2019detection.png},
	address = {A Coruna, Spain},
  project = {3Dreconstruction},
	code = {https://gitlab.com/Tachella/lidardetection},
	pdf = {https://ieeexplore.ieee.org/document/8903062},
	abs = {Single-photon light detection and ranging (Lidar) devices can be used to obtain range and reflectivity information from 3D scenes. However, reconstructing the 3D surfaces from the raw waveforms can be very challenging, in particular when the number of spurious background detections is large compared to the number of signal detections. This paper introduces a new and fast detection algorithm, which can be used to assess the presence of objects/surfaces in each waveform, allowing only the histograms where the imaged surfaces are present to be further processed. The method is compared to state-of-the-art 3D reconstruction methods using synthetic and real single-photon data and the results illustrate its benefits for fast and robust target detection using single-photon data.},
	month={Sep.},}

@inproceedings{tachella2019detection2,
	author={{Tachella}, Julian and {Altmann}, Yoann and {McLaughlin}, Stephen and {Tourneret}, Jean-Yves},
	title = {{On fast object detection using single-photon lidar data}},
	volume = {11138},
	booktitle = {Proc. SPIE Wavelets and Sparsity XVIII},
	publisher = {SPIE},
	address = {San Diego, USA},
	pages = {252 -- 261},
	year = {2019},
	month = {Sep},
  project = {3Dreconstruction},
	pdf = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11138/111380T/On-fast-object-detection-using-single-photon-lidar-data/10.1117/12.2527685.short},
	abstract = {Light detection and ranging (Lidar) systems based on single-photon detection can be used to obtain range and reflectivity information from 3D scenes with high range resolution. However, reconstructing the 3D surfaces from the raw single-photon waveforms is challenging, in particular when a limited number of photons is detected and when the ratio of spurious background detection events is large. This paper reviews a set of fast detection algorithms, which can be used to assess the presence of objects/surfaces in each waveform, allowing only the histograms where the imaged surfaces are present to be further processed. The original method we recently proposed is extended here using a multiscale approach to further reduce the computational complexity of the detection process. The proposed methods are compared to state-of-the-art 3D reconstruction methods using synthetic and real single-photon data and the results illustrate their benefits for fast and robust target detection.},
	code = {https://gitlab.com/Tachella/lidardetection},
	doi = {10.1117/12.2527685},
}


@inproceedings{tachella2018mcmc,
	author={{Tachella}, Julian and {Altmann}, Pereyra, Marcelo and Yoann and {McLaughlin}, Stephen and {Tourneret}, Jean-Yves},
	booktitle={Proc. 26th Eur. Signal Process. Conf. (EUSIPCO)},
	title={Bayesian Restoration of High-Dimensional Photon-Starved Images},
	year={2018},
	volume={},
	number={},
	pages={747-751},
	doi={10.23919/EUSIPCO.2018.8553175},
	ISSN={2219-5491},
	address = {Rome, Italy},
	pdf = {https://ieeexplore.ieee.org/abstract/document/8553175},
	abbr = {tachella2018mcmc.png},
	abstract = {This paper investigates different algorithms to perform image restoration from single-photon measurements corrupted with Poisson noise. The restoration problem is formulated in a Bayesian framework and several state-of-the-art Monte Carlo samplers are considered to estimate the unknown image and quantify its uncertainty. The different samplers are compared through a series of experiments conducted with synthetic images. The results demonstrate the scaling properties of the proposed samplers as the dimensionality of the problem increases and the number of photons decreases. Moreover, our experiments show that for a certain photon budget (i.e., acquisition time of the imaging device), downsampling the observations can yield better reconstruction results.},
	month={Sep.},}
