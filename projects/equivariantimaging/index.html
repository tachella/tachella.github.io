<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unsupervised Learning to Solve Inverse Problems | Julian Tachella </title> <meta name="author" content="Julian Tachella"> <meta name="description" content="Machine learning research scientist at CNRS. "> <meta name="keywords" content="machine learning, signal processing, computational imaging, CNRS, deep learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tachella.github.io/projects/equivariantimaging/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Julian</span> Tachella </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/Join%20Us/">Join Us </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unsupervised Learning to Solve Inverse Problems</h1> <p class="post-description"></p> </header> <article> <p>Inverse problems are ubiquitous in signal and image processing. In most applications, we need to reconstruct an underlying signal \(x\in\mathbb{R}^{n}\), from some measurements \(y\in\mathbb{R}^{m}\), that is, invert the forward measurement process, \begin{equation} y = Ax+n \end{equation} where \(n\) represents some noise and \(A\) is the forward operator. Due to the ill-posed nature of \(A\) (we generally have \(m&lt;n\)) and noise, there are multiple possible solutions \(x\) for a given \(y\). Fortunately, the set of plausible (natural) signals \(x\) lie in a small low-dimensional set \(\mathcal{X}\) of the whole of \(\mathbb{R}^{n}\), so we can have a unique \(x\) for a given \(y\).</p> <p>The traditional approach is to build a mathematical model to describe \(\mathcal{X}\) leveraging some prior knowledge about the underlying signals (e.g. natural images can be described as piecewise smooth). However, this a hard task which is problem-dependent and it is generally a loose description of the true \(\mathcal{X}\).</p> <p>In recent years, an alternative approach is to learn inverse mapping from \(y\mapsto x\) directly from training data, bypassing the need to design a prior model. Fuelled by the powerful learning bias of deep convolutional neural networks (interest readers can have a look at my previous post about understanding this implicit bias), the goal is to learn a function \(x=f(y)\) from training pairs \((x_i,y_i)\). The fundamental limitation of this approach is that in many real world applications we can only access \(y\). Training only with the \(y_i\) (enforcing measurement consistency) accounts to finding an \(f\) such that \(y=A f(y)\). Unfortunately this is doomed to fail, as there are infinite possible functions \(f\) that can fit the measurements perfectly well! This is because any \(f\) can output any value in the nullspace of \(A\) and still achieve measurement consistency. In other words, this fundamental limitation is a chicken-and-egg problem: we cannot learn to solve an inverse problem without solving it first to obtain the ground-truth training data!</p> <p>In <a class="citation" href="#chen2021equivariant">(Chen et al., 2021)</a>, we show that this problem can be overcome by adding a small assumption to the underlying set of signals \(\mathcal{X}\): invariance. It is well-known that most natural signals posses some kind of invariance. For example, images are generally invariant to shifts or rotations. Hence, the whole sensing process \(x = (f \circ A) (x)\) is necessarily an equivariant function, that is, given a transformation \(T_g\) (e.g. a shift), we have that \begin{equation} T_gx = (f\circ A) (T_gx). \end{equation} The invariance gives us information of the nullspace of A, which boils down to the following observation: \begin{equation} y=Ax = AT_g x’ = A_g x’ \end{equation} which just relies on the fact that \(x'= T_gx\) is another valid signal. Hence we can see beyond the range space of \(A\), as we have an implicit access to multiple different operators \(A_g = AT_g\) for all possible transformations \(T_1,\dots,T_{G}\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ei_iccv-480.webp 480w,/assets/img/ei_iccv-800.webp 800w,/assets/img/ei_iccv-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ei_iccv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Learning to image from only measurements. Training an imaging network through just measurement consistency (MC) does not significantly improve the reconstruction over the simple pseudo-inverse. However, by enforcing invariance in the reconstructed image set, equivariant imaging (EI) performs almost as well as a fully supervised network. Top: sparse view CT reconstruction, Bottom: pixel inpainting. PSNR is shown in top right corner of the images </div> <p><a class="citation" href="#chen2021equivariant">(Chen et al., 2021)</a> shows that the invariance constraint on \((f\circ A)\) can be easily incorporated as an additional loss term when training a deep network. In <a class="citation" href="#chen2021robust">(Chen et al., 2022)</a> we extended the unsupervised method to account for noise. The method builds an unsupervised loss using Stein’s unbiased risk (SURE) estimator, which approximates the noiseless measurement consistency.</p> <p>Experiments in <a class="citation" href="#chen2021equivariant">(Chen et al., 2021)</a> and <a class="citation" href="#chen2021equivariant">(Chen et al., 2021)</a> show that for the computed tomography and inpaiting problems, the equivariant learning approach (only having access to measurements \(y_i\)) performs as well as the fully supervised case i.e. having training pairs with ground-truth data \((x_i,y_i)\), by-passing the fundamental limitation of learning to solve inverse problems.</p> <h2 id="theory">Theory</h2> <p>Despite the good empirical results, a few important theoretical questions arise: <strong>When is unsupervised learning possible?</strong> How big has the group invariance has to be? How many measurements per observation do we need?</p> <p>We provide answers to these questions in <a class="citation" href="#tachella2022sampling">(Tachella et al., 2023)</a>:</p> <h3 id="necessary-conditions">Necessary Conditions</h3> <p>In order to learn from measurement data alone, we need that the set range spaces of virtual operators span the full ambient space \(\mathbb{R}^{n}\), i.e.,</p> \[\begin{equation}\label{eq:necessary} \text{rank}\begin{bmatrix} AT_1 \\ \vdots \\ AT_G \end{bmatrix} = n \end{equation}\] <p>This condition requires that \(m \geq \max_j c_j/s_j\) where \({s_j}\) and \({c_j}\) are the dimension and multiplicities of the irreducible representations of the group action. Most group symmetries (translations, reflections or rotations of a signal) appearing in practice have \(\max_j c_j/s_j=n/G\). In this case, we need at least \begin{equation} m \geq n/G \end{equation} measurements.</p> <p>Moreover, condition \eqref{eq:necessary} requires that the forward operator \(A\) is <strong>not</strong> equivariant to the group action. Otherwise, the concatenation of \(AT_1,\dots, AT_{G}\) has rank \(m&lt;n\).</p> <h3 id="sufficient-condition">Sufficient Condition</h3> <p>In order to guarantee unique model recovery, we need to take into account the dimension of the signal set \(\mathcal{X}\). Let \(k\) be the box-counting dimension of \(\mathcal{X}\) and let \(G\) be a cyclic group where \(\{c_j\}\) denote th multiplicities of the irreducible representations. Then, almost every forward operator \(A\in \mathbb{R}^{m\times n}\) with \(m&gt; 2k + 1 + \max_j c_j\). Most cyclic group symmetries (translations, reflections or rotations of a signal) appearing in practice have \(\max_j c_j=n/G\). In these cases we have that fully self-supervised learning is possible by almost every \(A\) with \begin{equation} m&gt; 2k + 1 + n/G \end{equation} measurements.</p> <h4 id="multiple-operators">Multiple operators</h4> <p>If the signal set is not group invariant, but we observe measurements via different operators \(A_1,\dots,A_G\), then unsupervised from measurement data alone is possible. In this case the necessary condition on the number of measurements is \(m\geq n/G\), and the sufficient condition is \(m&gt;n/G+k\). These results are included in <a class="citation" href="#tachella2022samplingshort">(Tachella et al., 2022)</a>.</p> <h3 id="related-papers">Related papers</h3> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/m_regimes_website-480.webp 480w,/assets/img/publication_preview/m_regimes_website-800.webp 800w,/assets/img/publication_preview/m_regimes_website-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/m_regimes_website.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="m_regimes_website.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tachella2022sampling" class="col-sm-8"> <div class="title">Sensing Theorems for Unsupervised Learning in Linear Inverse Problems</div> <div class="author"> <em>Julian Tachella</em>, Dongdong Chen , and Mike Davies </div> <div class="periodical"> <em>Journal of Machine Learning Research (JMLR)</em>, Jan 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2203.12513" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="http://jmlr.org/papers/v24/22-0315.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Solving a linear inverse problem requires knowledge about the underlying signal model. In many applications, this model is a priori unknown and has to be learned from data. However, it is impossible to learn the model using observations obtained via a single incomplete measurement operator, as there is no information outside the range of the inverse operator, resulting in a chicken-and-egg problem: to learn the model we need reconstructed signals, but to reconstruct the signals we need to know the model. Two ways to overcome this limitation are using multiple measurement operators or assuming that the signal model is invariant to a certain group action. In this paper, we present necessary and sufficient sensing conditions for learning the signal model from partial measurements which only depend on the dimension of the model, and the number of operators or properties of the group action that the model is invariant to. As our results are agnostic of the learning algorithm, they shed light into the fundamental limitations of learning from incomplete data and have implications in a wide range set of practical algorithms, such as dictionary learning, matrix completion and deep neural networks.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chen2021robust-480.webp 480w,/assets/img/publication_preview/chen2021robust-800.webp 800w,/assets/img/publication_preview/chen2021robust-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/chen2021robust.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chen2021robust.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2021robust" class="col-sm-8"> <div class="title">Robust Equivariant Imaging: a fully unsupervised framework for learning to image from noisy and partial measurements</div> <div class="author"> Dongdong Chen , <em>Julian Tachella</em>, and Mike E Davies </div> <div class="periodical"> <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2111.12855" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://github.com/edongdongchen/REI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Deep networks provide state-of-the-art performance in multiple imaging inverse problems ranging from medical imaging to computational photography. However, most existing networks are trained with clean signals which are often hard or impossible to obtain. Equivariant imaging (EI) is a recent self-supervised learning framework that exploits the group invariance present in signal distributions to learn a reconstruction function from partial measurement data alone. While EI results are impressive, its performance degrades with increasing noise. In this paper, we propose a Robust Equivariant Imaging (REI) framework which can learn to image from noisy partial measurements alone. The proposed method uses Stein’s Unbiased Risk Estimator (SURE) to obtain a fully unsupervised training loss that is robust to noise. We show that REI leads to considerable performance gains on linear and nonlinear inverse problems, thereby paving the way for robust unsupervised imaging with deep networks.</p> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/27iWnWEbQvA" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tachella2022sampling.PNG-480.webp 480w,/assets/img/publication_preview/tachella2022sampling.PNG-800.webp 800w,/assets/img/publication_preview/tachella2022sampling.PNG-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tachella2022sampling.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tachella2022sampling.PNG" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tachella2022samplingshort" class="col-sm-8"> <div class="title">Unsupervised Learning From Incomplete Measurements for Inverse Problems</div> <div class="author"> <em>Julian Tachella</em>, Dongdong Chen , and Mike Davies </div> <div class="periodical"> <em>NeurIPS 2022</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2201.12151" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/edongdongchen/MOI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In many real-world inverse problems, only incomplete measurement data are available for training which can pose a problem for learning a reconstruction function. Indeed, unsupervised learning using a fixed incomplete measurement process is impossible in general, as there is no information in the nullspace of the measurement operator. This limitation can be overcome by using measurements from multiple operators. While this idea has been successfully applied in various applications, a precise characterization of the conditions for learning is still lacking. In this paper, we fill this gap by presenting necessary and sufficient conditions for learning the underlying signal model needed for reconstruction which indicate the interplay between the number of distinct measurement operators, the number of measurements per operator, the dimension of the model and the dimension of the signals. Furthermore, we propose a novel and conceptually simple unsupervised learning loss which only requires access to incomplete measurement data and achieves a performance on par with supervised learning when the sufficient condition is verified. We validate our theoretical bounds and demonstrate the advantages of the proposed unsupervised loss compared to previous methods via a series of experiments on various imaging inverse problems, such as accelerated magnetic resonance imaging, compressed sensing and image inpainting. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chen2021equivariant-480.webp 480w,/assets/img/publication_preview/chen2021equivariant-800.webp 800w,/assets/img/publication_preview/chen2021equivariant-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/chen2021equivariant.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chen2021equivariant.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2021equivariant" class="col-sm-8"> <div class="title">Equivariant Imaging: Learning Beyond the Range Space</div> <div class="author"> Dongdong Chen , <em>Julian Tachella</em>, and Mike E Davies </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em> , Mar 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://github.com/edongdongchen/EI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In various imaging problems, we only have access to compressed measurements of the underlying signals, hindering most learning-based strategies which usually require pairs of signals and associated measurements for training. Learning only from compressed measurements is impossible in general, as the compressed observations do not contain information outside the range of the forward sensing operator. We propose a new end-to-end self-supervised framework that overcomes this limitation by exploiting the equivariances present in natural signals. Our proposed learning strategy performs as well as fully supervised methods. Experiments demonstrate the potential of this framework on inverse problems including sparse-view X-ray computed tomography on real clinical data and image inpainting on natural images.</p> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/wGxW5bcCdxo" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Julian Tachella. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>