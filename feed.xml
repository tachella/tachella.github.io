<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://tachella.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tachella.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-11T07:21:06+00:00</updated><id>https://tachella.github.io/feed.xml</id><title type="html">blank</title><subtitle>Machine learning research scientist at CNRS. </subtitle><entry><title type="html">Self-supervised learning for imaging tutorial</title><link href="https://tachella.github.io/blog/selfsuptutorial/" rel="alternate" type="text/html" title="Self-supervised learning for imaging tutorial"/><published>2024-08-27T19:22:00+00:00</published><updated>2024-08-27T19:22:00+00:00</updated><id>https://tachella.github.io/blog/selfsuptutorial</id><content type="html" xml:base="https://tachella.github.io/blog/selfsuptutorial/"><![CDATA[<p>This page contains information about the tutorial on self-supervised learning for imaging, given by <a href="https://www.eng.ed.ac.uk/about/people/professor-michael-e-davies">Mike Davies</a> and I. The tutorial is part of the <a href="https://eusipcolyon.sciencesconf.org/resource/page/id/28">EUSIPCO 2024 conference</a> on the 26/08/2024 in Lyon, France, and the MAC-MIGS doctoral school given at the University of Edinburgh in February 2025.</p> <p><a href="https://youtube.com/playlist?list=PLrflIVF5S9hDfFKdH3yAgNrLnP2JF6WBN&amp;si=t46JCApNlTSsuNDz">YouTube Playlist</a></p> <p><strong>Videos &amp; Slides</strong>:</p> <ol> <li>Introduction. <a href="https://youtu.be/gf-WCHXAdfk">video</a>, <a href="/assets/pdf/part1.pdf">slides</a></li> <li>Learning from noisy data <a href="https://youtu.be/dxgvrooTZqQ">video</a>, <a href="/assets/pdf/part2.pdf">slides</a></li> <li>Learning from incomplete operators. <a href="https://youtu.be/bIjEmd0kGN8">video</a>, <a href="/assets/pdf/part3.pdf">slides</a></li> <li>Equivariant imaging. <a href="https://youtu.be/7M52ensBFxM">video</a>, <a href="/assets/pdf/part4.pdf">slides</a></li> <li>Identification theory. <a href="https://youtu.be/Z1N7o9PlRIc">video</a>, <a href="/assets/pdf/part5.pdf">slides</a></li> <li>Perspectives. <a href="https://youtu.be/pk6Sl52c9x4">video</a>, <a href="/assets/pdf/part6.pdf">slides</a></li> </ol> <p>Bonus videos:</p> <ul> <li><a href="https://youtu.be/p0KXiReF3sg?si=mw5rMkYFAkoYFr4Q">UNSURE: Unknown Noise level Stein’s Unbiased Risk Estimate</a></li> <li><a href="https://youtu.be/C9IHJm_Ie2k?si=FBOzZ9tDu63k_Hdz">Generalized Recorrupted2Recorrupted</a></li> <li><a href="https://youtu.be/3JtrPP7qUCY?si=a3304Ad6viMwF_e7">Equivariant Imaging with real-world group transforms</a></li> </ul> <p><strong>Code</strong>: We will follow the Google Colab demo in <a href="https://colab.research.google.com/drive/1_dlXdNbgwg5u7_OAl29WiMRMOybnkCIo?usp=sharing">this link</a>. Advanced self-supervised learning <a href="https://andrewwango.github.io/deepinv-selfsup-fastmri">MRI benchmarking code</a> by Andrew Wang. More self-supervised learning demos can be found on the deepinverse website <a href="https://deepinv.github.io/deepinv/auto_examples/index.html#self-supervised-learning">here</a>.</p> <p><strong>Abstract</strong>: This tutorial will cover core concepts and recent advances in the emerging field of self-supervised learning methods for solving imaging inverse problems with deep neural networks. Self-supervised learning is a fundamental tool deploying deep learning solutions in scientific and medical imaging applications where obtaining a large dataset of ground-truth images is very expensive or impossible. The tutorial will provide a comprehensive summary of different self-supervised methods, discuss their theoretical underpinnings and present practical self-supervised imaging applications.</p> <h3 id="references">References</h3> <p>List of references mentioned in the tutorial by topic.</p> <h4 id="part-i-introduction">Part I: Introduction</h4> <ul> <li>Zbontar, Jure, et al. “fastMRI: An open dataset and benchmarks for accelerated MRI.” arXiv preprint arXiv:1811.08839 (2018).</li> <li>Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. “Deep image prior.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.</li> <li>Jin K. H., McCann M. T., Froustey E., Unser M., Deep convolutional neural network for inverse problems in imaging. IEEE Trans. Im. Proc., 2017.</li> <li>Monga V., Li Y., Eldar Y. C., Algorithm unrolling: interpretable, efficient deep learning for signal and image processing. IEEE Sig. Proc. Mag., 2021.</li> <li>Y. Zhu et al., “Denoising Diffusion Models for Plug-and-Play Image Restoration,” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Vancouver, BC, Canada, 2023, pp. 1219-1229.</li> </ul> <h4 id="part-ii-learning-from-noisy-data">Part II: Learning from noisy data</h4> <h5 id="noise2noise-methods">Noise2Noise methods</h5> <ul> <li>Mallows, Colin L. “Some comments on Cp.” Technometrics 15.4 (1973): 661-675.</li> <li>Lehtinen, Jaakko, et al. “Noise2noise: Learning image restoration without clean data.” Proceedings of the 35th International Conference on Machine Learning. 2018.</li> </ul> <h5 id="sure-methods">SURE methods</h5> <ul> <li>Stein, Charles M. “Estimation of the mean of a multivariate normal distribution.” The annals of Statistics (1981): 1135-1151.</li> <li>Breiman, Leo. “The little bootstrap and other methods for dimensionality selection in regression: X-fixed prediction error.” Journal of the American Statistical Association 87.419 (1992): 738-754.</li> <li>Hudson, H. Malcolm. “A natural identity for exponential families with applications in multiparameter estimation.” The Annals of Statistics 6.3 (1978): 473-484.</li> <li>Ramani, Sathish, Thierry Blu, and Michael Unser. “Monte-Carlo SURE: A black-box optimization of regularization parameters for general denoising algorithms.” IEEE Transactions on image processing 17.9 (2008): 1540-1554.</li> <li>Eldar, Yonina C. “Generalized SURE for exponential families: Applications to regularization.” IEEE Transactions on Signal Processing 57.2 (2008): 471-481.</li> <li>Metzler, Christopher A., et al. “Unsupervised learning with Stein’s unbiased risk estimator.” arXiv preprint arXiv:1805.10531 (2018).</li> <li>Kim, Kwanyoung, and Jong Chul Ye. “Noise2score: tweedies approach to self-supervised image denoising without clean images.” Advances in Neural Information Processing Systems 34 (2021): 864-874.</li> <li>Tachella, Julian, Mike Davies, and Laurent Jacques. “UNSURE: Unknown Noise level Stein’s Unbiased Risk Estimator.” ICLR (2024).</li> </ul> <h5 id="noisier2noise-methods">Noisier2Noise methods</h5> <ul> <li>Moran, Nick, et al. “Noisier2noise: Learning to denoise from unpaired noisy data.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</li> <li>Pang, Tongyao, et al. “Recorrupted-to-recorrupted: Unsupervised deep learning for image denoising.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.</li> <li>Oliveira, Natalia L., Jing Lei, and Ryan J. Tibshirani. “Unbiased risk estimation in the normal means problem via coupled bootstrap techniques.” arXiv preprint arXiv:2111.09447 (2021).</li> <li>Oliveira, Natalia L., Jing Lei, and Ryan J. Tibshirani. “Unbiased test error estimation in the poisson means problem via coupled bootstrap techniques.” arXiv preprint arXiv:2212.01943 (2022).</li> <li>Monroy, Brayan, Jorge Bacca, and Julian Tachella. “Generalized Recorrupted-to-Recorrupted: Self-Supervised Learning Beyond Gaussian Noise.” CVPR (2025).</li> </ul> <h5 id="noise2void-and-cross-validation-methods">Noise2Void and cross-validation methods</h5> <ul> <li>Efron, Bradley. “The estimation of prediction error: covariance penalties and cross-validation.” Journal of the American Statistical Association 99.467 (2004): 619-632.</li> <li>Krull, Alexander, Tim-Oliver Buchholz, and Florian Jug. “Noise2void-learning denoising from single noisy images.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.</li> <li>Batson, Joshua, and Loic Royer. “Noise2self: Blind denoising by self-supervision.” International Conference on Machine Learning. PMLR, 2019.</li> <li>Huang, Tao, et al. “Neighbor2neighbor: Self-supervised denoising from single noisy images.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.</li> <li>Hendriksen, Allard Adriaan, Daniel Maria Pelt, and K. Joost Batenburg. “Noise2inverse: Self-supervised deep convolutional denoising for tomography.” IEEE Transactions on Computational Imaging 6 (2020): 1320-1335.</li> </ul> <h5 id="blind-spot-networks">Blind spot networks</h5> <ul> <li>Laine, Samuli, et al. “High-quality self-supervised deep image denoising.” Advances in Neural Information Processing Systems 32 (2019).</li> <li>W Lee, S Son, K M Lee; AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 17725-17734</li> </ul> <h4 id="part-iii-learning-from-incomplete-operators">Part III: Learning from incomplete operators</h4> <ul> <li>Liu, Jiaming, et al. “RARE: Image reconstruction using deep priors learned without groundtruth.” IEEE Journal of Selected Topics in Signal Processing 14.6 (2020): 1088-1099.</li> <li>Tachella, Julian, Dongdong Chen, and Mike Davies. “Unsupervised learning from incomplete measurements for inverse problems.” Advances in Neural Information Processing Systems 35 (2022): 4983-4995.</li> <li>Yaman, Burhaneddin, et al. “Self supervised learning of physics guided reconstruction neural networks without fully sampled reference data.” Magnetic resonance in medicine 84.6 (2020): 3172-3191.</li> <li>Daras, Giannis, et al. “Ambient diffusion: Learning clean distributions from corrupted data.” Advances in Neural Information Processing Systems 36 (2024).</li> <li>Gan W. et al., Self-Supervised Deep Equilibrium Models with Theoretical Guarantees and Applications to MRI Reconstruction. IEEE Trans. Comp Imag., 2023.</li> <li>C. Millard and M. Chiew, “A Theoretical Framework for Self-Supervised MR Image Reconstruction Using Sub-Sampling via Variable Density Noisier2Noise,” in IEEE Transactions on Computational Imaging, vol. 9, pp. 707-720, 2023.</li> </ul> <h4 id="part-iv-equivariant-imaging">Part IV: Equivariant Imaging</h4> <ul> <li>Chen, Dongdong, Julian Tachella, and Mike E. Davies. “Equivariant imaging: Learning beyond the range space.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.</li> <li>Chen, Dongdong, Julian Tachella, and Mike E. Davies. “Robust equivariant imaging: a fully unsupervised framework for learning to image from noisy and partial measurements.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.</li> <li>Scanvic, Jeremy, et al. “Self-supervised learning for image super-resolution and deblurring.” arXiv preprint arXiv:2312.11232 (2023).</li> <li>Wang, Andrew, and Mike Davies. “Perspective-equivariant imaging: an unsupervised framework for multispectral pansharpening.” ECCV Workshops (2024).</li> <li>Wang, Andrew, and Mike Davies. “Fully Unsupervised Dynamic MRI Reconstruction via Diffeo-Temporal Equivariance.” ISBI (2025).</li> </ul> <h4 id="part-v-identification-theory">Part V: Identification theory</h4> <ul> <li>Sauer, Tim, James A. Yorke, and Martin Casdagli. “Embedology.” Journal of statistical Physics 65 (1991): 579-616.</li> <li>Tachella, Julian, Dongdong Chen, and Mike Davies. “Sensing theorems for unsupervised learning in linear inverse problems.” Journal of Machine Learning Research 24.39 (2023): 1-45.</li> <li>Cramer, Harald; Wold, Herman (1936). “Some Theorems on Distribution Functions”. Journal of the London Mathematical Society. 11 (4): 290-294.</li> <li>Bourrier A., Davies M. E., Peleg T., Perez P., Gribonval R. Fundamental Performance Limits for Ideal Decoders in High-Dimensional Linear Inverse Problems. IEEE Trans. Inf. Thy., 2014.</li> </ul> <h4 id="part-vi-perspectives">Part VI: Perspectives</h4> <ul> <li>Bora, Ashish, Eric Price, and Alexandros G. Dimakis. “AmbientGAN: Generative models from lossy measurements.” International conference on learning representations. 2018.</li> <li>Hermosilla, Pedro, Tobias Ritschel, and Timo Ropinski. “Total denoising: Unsupervised learning of 3D point cloud cleaning.” Proceedings of the IEEE/CVF international conference on computer vision. 2019.</li> <li>Tachella, Julian, and Laurent Jacques. “Learning to reconstruct signals from binary measurements.” arXiv preprint arXiv:2303.08691 (2023).</li> <li>Bellec, Pierre C., and Cun-Hui Zhang. “Second-order Stein: SURE for SURE and other applications in high-dimensional inference.” The Annals of Statistics 49.4 (2021): 1864-1903.</li> <li>Tachella, Julian, and Marcelo Pereyra. “Equivariant bootstrapping for uncertainty quantification in imaging inverse problems.” AISTATS (2024).</li> </ul>]]></content><author><name></name></author><category term="self-supervised learning"/><summary type="html"><![CDATA[information about EUSIPCO'24 tutorial]]></summary></entry><entry><title type="html">Unsupervised Learning to Solve Inverse Problems</title><link href="https://tachella.github.io/blog/equivariantimaging/" rel="alternate" type="text/html" title="Unsupervised Learning to Solve Inverse Problems"/><published>2022-05-10T19:22:00+00:00</published><updated>2022-05-10T19:22:00+00:00</updated><id>https://tachella.github.io/blog/equivariantimaging</id><content type="html" xml:base="https://tachella.github.io/blog/equivariantimaging/"><![CDATA[<p>Inverse problems are ubiquitous in signal and image processing. In most applications, we need to reconstruct an underlying signal \(x\in\mathbb{R}^{n}\), from some measurements \(y\in\mathbb{R}^{m}\), that is, invert the forward measurement process, \begin{equation} y = Ax+n \end{equation} where \(n\) represents some noise and \(A\) is the forward operator. Due to the ill-posed nature of \(A\) (we generally have \(m&lt;n\)) and noise, there are multiple possible solutions \(x\) for a given \(y\). Fortunately, the set of plausible (natural) signals \(x\) lie in a small low-dimensional set \(\mathcal{X}\) of the whole of \(\mathbb{R}^{n}\), so we can have a unique \(x\) for a given \(y\).</p> <p>The traditional approach is to build a mathematical model to describe \(\mathcal{X}\) leveraging some prior knowledge about the underlying signals (e.g. natural images can be described as piecewise smooth). However, this a hard task which is problem-dependent and it is generally a loose description of the true \(\mathcal{X}\).</p> <p>In recent years, an alternative approach is to learn inverse mapping from \(y\mapsto x\) directly from training data, bypassing the need to design a prior model. Fuelled by the powerful learning bias of deep convolutional neural networks (interest readers can have a look at my previous post about understanding this implicit bias), the goal is to learn a function \(x=f(y)\) from training pairs \((x_i,y_i)\). The fundamental limitation of this approach is that in many real world applications we can only access \(y\). Training only with the \(y_i\) (enforcing measurement consistency) accounts to finding an \(f\) such that \(y=A f(y)\). Unfortunately this is doomed to fail, as there are infinite possible functions \(f\) that can fit the measurements perfectly well! This is because any \(f\) can output any value in the nullspace of \(A\) and still achieve measurement consistency. In other words, this fundamental limitation is a chicken-and-egg problem: we cannot learn to solve an inverse problem without solving it first to obtain the ground-truth training data!</p> <p>In <a class="citation" href="#chen2021equivariant">(Chen et al., 2021)</a>, we show that this problem can be overcome by adding a small assumption to the underlying set of signals \(\mathcal{X}\): invariance. It is well-known that most natural signals posses some kind of invariance. For example, images are generally invariant to shifts or rotations. Hence, the whole sensing process \(x = (f \circ A) (x)\) is necessarily an equivariant function, that is, given a transformation \(T_g\) (e.g. a shift), we have that \begin{equation} T_gx = (f\circ A) (T_gx). \end{equation} The invariance gives us information of the nullspace of A, which boils down to the following observation: \begin{equation} y=Ax = AT_g x’ = A_g x’ \end{equation} which just relies on the fact that \(x'= T_gx\) is another valid signal. Hence we can see beyond the range space of \(A\), as we have an implicit access to multiple different operators \(A_g = AT_g\) for all possible transformations \(T_1,\dots,T_{G}\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ei_iccv-480.webp 480w,/assets/img/ei_iccv-800.webp 800w,/assets/img/ei_iccv-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ei_iccv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Learning to image from only measurements. Training an imaging network through just measurement consistency (MC) does not significantly improve the reconstruction over the simple pseudo-inverse. However, by enforcing invariance in the reconstructed image set, equivariant imaging (EI) performs almost as well as a fully supervised network. Top: sparse view CT reconstruction, Bottom: pixel inpainting. PSNR is shown in top right corner of the images </div> <p><a class="citation" href="#chen2021equivariant">(Chen et al., 2021)</a> shows that the invariance constraint on \((f\circ A)\) can be easily incorporated as an additional loss term when training a deep network. In <a class="citation" href="#chen2021robust">(Chen et al., 2022)</a> we extended the unsupervised method to account for noise. The method builds an unsupervised loss using Stein’s unbiased risk (SURE) estimator, which approximates the noiseless measurement consistency.</p> <p>Experiments in <a class="citation" href="#chen2021equivariant">(Chen et al., 2021)</a> and <a class="citation" href="#chen2021equivariant">(Chen et al., 2021)</a> show that for the computed tomography and inpaiting problems, the equivariant learning approach (only having access to measurements \(y_i\)) performs as well as the fully supervised case i.e. having training pairs with ground-truth data \((x_i,y_i)\), by-passing the fundamental limitation of learning to solve inverse problems.</p> <h2 id="theory">Theory</h2> <p>Despite the good empirical results, a few important theoretical questions arise: <strong>When is unsupervised learning possible?</strong> How big has the group invariance has to be? How many measurements per observation do we need?</p> <p>We provide answers to these questions in <a class="citation" href="#tachella2022sampling">(Tachella et al., 2023)</a>:</p> <h3 id="necessary-conditions">Necessary Conditions</h3> <p>In order to learn from measurement data alone, we need that the set range spaces of virtual operators span the full ambient space \(\mathbb{R}^{n}\), i.e.,</p> \[\begin{equation}\label{eq:necessary} \text{rank}\begin{bmatrix} AT_1 \\ \vdots \\ AT_G \end{bmatrix} = n \end{equation}\] <p>This condition requires that \(m \geq \max_j c_j/s_j\) where \({s_j}\) and \({c_j}\) are the dimension and multiplicities of the irreducible representations of the group action. Most group symmetries (translations, reflections or rotations of a signal) appearing in practice have \(\max_j c_j/s_j=n/G\). In this case, we need at least \begin{equation} m \geq n/G \end{equation} measurements.</p> <p>Moreover, condition \eqref{eq:necessary} requires that the forward operator \(A\) is <strong>not</strong> equivariant to the group action. Otherwise, the concatenation of \(AT_1,\dots, AT_{G}\) has rank \(m&lt;n\).</p> <h3 id="sufficient-condition">Sufficient Condition</h3> <p>In order to guarantee unique model recovery, we need to take into account the dimension of the signal set \(\mathcal{X}\). Let \(k\) be the box-counting dimension of \(\mathcal{X}\) and let \(G\) be a cyclic group where \(\{c_j\}\) denote th multiplicities of the irreducible representations. Then, almost every forward operator \(A\in \mathbb{R}^{m\times n}\) with \(m&gt; 2k + 1 + \max_j c_j\). Most cyclic group symmetries (translations, reflections or rotations of a signal) appearing in practice have \(\max_j c_j=n/G\). In these cases we have that fully self-supervised learning is possible by almost every \(A\) with \begin{equation} m&gt; 2k + 1 + n/G \end{equation} measurements.</p> <h4 id="multiple-operators">Multiple operators</h4> <p>If the signal set is not group invariant, but we observe measurements via different operators \(A_1,\dots,A_G\), then unsupervised from measurement data alone is possible. In this case the necessary condition on the number of measurements is \(m\geq n/G\), and the sufficient condition is \(m&gt;n/G+k\). These results are included in <a class="citation" href="#tachella2022samplingshort">(Tachella et al., 2022)</a>.</p> <h3 id="related-papers">Related papers</h3> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/m_regimes_website-480.webp 480w,/assets/img/publication_preview/m_regimes_website-800.webp 800w,/assets/img/publication_preview/m_regimes_website-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/m_regimes_website.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="m_regimes_website.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="tachella2022sampling" class="col-sm-8"> <div class="title">Sensing Theorems for Unsupervised Learning in Linear Inverse Problems</div> <div class="author"> <em>Julian Tachella</em>,&nbsp;Dongdong Chen ,&nbsp;and&nbsp;Mike Davies </div> <div class="periodical"> <em>Journal of Machine Learning Research (JMLR)</em>, Jan 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2203.12513" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a href="http://jmlr.org/papers/v24/22-0315.html" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Solving a linear inverse problem requires knowledge about the underlying signal model. In many applications, this model is a priori unknown and has to be learned from data. However, it is impossible to learn the model using observations obtained via a single incomplete measurement operator, as there is no information outside the range of the inverse operator, resulting in a chicken-and-egg problem: to learn the model we need reconstructed signals, but to reconstruct the signals we need to know the model. Two ways to overcome this limitation are using multiple measurement operators or assuming that the signal model is invariant to a certain group action. In this paper, we present necessary and sufficient sensing conditions for learning the signal model from partial measurements which only depend on the dimension of the model, and the number of operators or properties of the group action that the model is invariant to. As our results are agnostic of the learning algorithm, they shed light into the fundamental limitations of learning from incomplete data and have implications in a wide range set of practical algorithms, such as dictionary learning, matrix completion and deep neural networks.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chen2021robust-480.webp 480w,/assets/img/publication_preview/chen2021robust-800.webp 800w,/assets/img/publication_preview/chen2021robust-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/chen2021robust.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chen2021robust.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="chen2021robust" class="col-sm-8"> <div class="title">Robust Equivariant Imaging: a fully unsupervised framework for learning to image from noisy and partial measurements</div> <div class="author"> Dongdong Chen ,&nbsp;<em>Julian Tachella</em>,&nbsp;and&nbsp;Mike E Davies </div> <div class="periodical"> <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2111.12855" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://github.com/edongdongchen/REI" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Deep networks provide state-of-the-art performance in multiple imaging inverse problems ranging from medical imaging to computational photography. However, most existing networks are trained with clean signals which are often hard or impossible to obtain. Equivariant imaging (EI) is a recent self-supervised learning framework that exploits the group invariance present in signal distributions to learn a reconstruction function from partial measurement data alone. While EI results are impressive, its performance degrades with increasing noise. In this paper, we propose a Robust Equivariant Imaging (REI) framework which can learn to image from noisy partial measurements alone. The proposed method uses Stein’s Unbiased Risk Estimator (SURE) to obtain a fully unsupervised training loss that is robust to noise. We show that REI leads to considerable performance gains on linear and nonlinear inverse problems, thereby paving the way for robust unsupervised imaging with deep networks.</p> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/27iWnWEbQvA" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tachella2022sampling.PNG-480.webp 480w,/assets/img/publication_preview/tachella2022sampling.PNG-800.webp 800w,/assets/img/publication_preview/tachella2022sampling.PNG-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/tachella2022sampling.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tachella2022sampling.PNG" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="tachella2022samplingshort" class="col-sm-8"> <div class="title">Unsupervised Learning From Incomplete Measurements for Inverse Problems</div> <div class="author"> <em>Julian Tachella</em>,&nbsp;Dongdong Chen ,&nbsp;and&nbsp;Mike Davies </div> <div class="periodical"> <em>NeurIPS 2022</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2201.12151" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a href="https://github.com/edongdongchen/MOI" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>In many real-world inverse problems, only incomplete measurement data are available for training which can pose a problem for learning a reconstruction function. Indeed, unsupervised learning using a fixed incomplete measurement process is impossible in general, as there is no information in the nullspace of the measurement operator. This limitation can be overcome by using measurements from multiple operators. While this idea has been successfully applied in various applications, a precise characterization of the conditions for learning is still lacking. In this paper, we fill this gap by presenting necessary and sufficient conditions for learning the underlying signal model needed for reconstruction which indicate the interplay between the number of distinct measurement operators, the number of measurements per operator, the dimension of the model and the dimension of the signals. Furthermore, we propose a novel and conceptually simple unsupervised learning loss which only requires access to incomplete measurement data and achieves a performance on par with supervised learning when the sufficient condition is verified. We validate our theoretical bounds and demonstrate the advantages of the proposed unsupervised loss compared to previous methods via a series of experiments on various imaging inverse problems, such as accelerated magnetic resonance imaging, compressed sensing and image inpainting. </p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chen2021equivariant-480.webp 480w,/assets/img/publication_preview/chen2021equivariant-800.webp 800w,/assets/img/publication_preview/chen2021equivariant-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/chen2021equivariant.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chen2021equivariant.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="chen2021equivariant" class="col-sm-8"> <div class="title">Equivariant Imaging: Learning Beyond the Range Space</div> <div class="author"> Dongdong Chen ,&nbsp;<em>Julian Tachella</em>,&nbsp;and&nbsp;Mike E Davies </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em> , Mar 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Equivariant_Imaging_Learning_Beyond_the_Range_Space_ICCV_2021_paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://github.com/edongdongchen/EI" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>In various imaging problems, we only have access to compressed measurements of the underlying signals, hindering most learning-based strategies which usually require pairs of signals and associated measurements for training. Learning only from compressed measurements is impossible in general, as the compressed observations do not contain information outside the range of the forward sensing operator. We propose a new end-to-end self-supervised framework that overcomes this limitation by exploiting the equivariances present in natural signals. Our proposed learning strategy performs as well as fully supervised methods. Experiments demonstrate the potential of this framework on inverse problems including sparse-view X-ray computed tomography on real clinical data and image inpainting on natural images.</p> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/wGxW5bcCdxo" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> </div> </div> </li></ol> </div>]]></content><author><name></name></author><category term="self-supervised learning"/><summary type="html"><![CDATA[How to learn from incomplete data only]]></summary></entry><entry><title type="html">Understanding the Deep Image Prior</title><link href="https://tachella.github.io/blog/understandingcnns/" rel="alternate" type="text/html" title="Understanding the Deep Image Prior"/><published>2022-01-10T19:22:00+00:00</published><updated>2022-01-10T19:22:00+00:00</updated><id>https://tachella.github.io/blog/understandingcnns</id><content type="html" xml:base="https://tachella.github.io/blog/understandingcnns/"><![CDATA[<p>Convolutional neural networks (CNNs) are a well-established tool for solving computational imaging problems. It has been recently shown that, despite being highly overparameterized (more weights than pixels), networks trained with a single corrupted image can still perform as well as fully trained networks (a.k.a. the deep image prior). These results highlight that CNNs posses a very powerful learning bias towards natural images, which explains their great success in recent years. Multiple intriguing question arise:</p> <p><strong>What is the learning bias?</strong> Are neural networks performing something similar to other existing tools in signal processing? Is the existing theory able to explain this phenomenon?</p> <p>In <a class="citation" href="#tachella2021nonlocal">(Tachella et al., 2021)</a>, we make a first step towards answering these questions, using recent theoretical insights of infinitely wide networks (a.k.a. the neural tangent kernel), elucidating formal links between CNNs and well-known non-local patch denoisers, such as non-local means.</p> <p>Non-local means uses the following non-local similarity function:</p> \[k(y_i, y_j) = \exp(-||y_i-y_j||^2/\sigma^2)\] <p>where \(y_i\) and \(y_j\) are small image patches (e.g. \(5\times 5\) pixels) around the pixels \(i\) and \(j\). The filter matrix \(W\) is constructed as \(W = \text{diag}(\frac{1}{1^TK}) K\) and the simplest denoising procedure consists of applying \(W\) to the (vectorized) noisy image \(y\), that is \(\hat{z}=W y\). There are more sophisticated procedures such as twicing, where the filtering matrix is applied iteratively to the residual:</p> \[z^{k+1} = z^{k} + W(y-z^{k})\] <p>This procedure trades bias (over-smooth estimates) for variance (noisy estimates), and is stopped when a good balance is achieved. How does this relate to a convolutional neural network trained with a single image? It turns out that, as the network’s width increases, standard gradient descent optimization of the squared \(\ell_2\) loss follows the twicing process, with a (fixed!) filter matrix \(W=K\) where the pixel affinity function is available in closed form and only depends on the architecture of the network! For example, a simple single-hidden layer network with a filter of \(k\times k\) pixels, corresponds to a non-local similarity function</p> \[k(y_i, y_j) = \frac{||y_i|| ||y_j||}{\pi} (\sin\phi+(\pi-\phi)\cos\phi)\] <p>where \(\phi\) is the angle between patches \(y_i\) and \(y_j\) of \(k\times k\) pixels each. Hence, we can compute the implicit filter in closed-form, without need to train a very large network!</p> <p>Our analysis reveals that a neural network that, while the NTK theory accurately predicts the filter associated with networks trained using standard gradient descent, it falls short to explain the behavior of networks trained using the popular Adam optimizer. The latter achieves a larger change of weights in hidden layers, adapting the non-local filtering function during training. We evaluate our findings via extensive image denoising experiments. See the paper for more details!</p> <h3 id="related-papers">Related papers</h3> <div class="publications"> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tachella2021nonlocal-480.webp 480w,/assets/img/publication_preview/tachella2021nonlocal-800.webp 800w,/assets/img/publication_preview/tachella2021nonlocal-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/tachella2021nonlocal.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tachella2021nonlocal.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="tachella2021nonlocal" class="col-sm-8"> <div class="title">The Neural Tangent Link Between CNN Denoisers and Non-Local Filters</div> <div class="author"> <em>Julian Tachella</em>,&nbsp;Junqi Tang ,&nbsp;and&nbsp;Mike Davies </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Tachella_The_Neural_Tangent_Link_Between_CNN_Denoisers_and_Non-Local_Filters_CVPR_2021_paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="2020/ntd" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://gitlab.com/Tachella/neural_tangent_denoiser" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Convolutional Neural Networks (CNNs) are now a well-established tool for solving computational imaging problems. Modern CNN-based algorithms obtain state-of-the-art performance in diverse image restoration problems. Furthermore, it has been recently shown that, despite being highly overparameterized, networks trained with a single corrupted image can still perform as well as fully trained networks. We introduce a formal link between such networks through their neural tangent kernel (NTK), and well-known non-local filtering techniques, such as non-local means or BM3D. The filtering function associated with a given network architecture can be obtained in closed form without need to train the network, being fully characterized by the random initialization of the network weights. While the NTK theory accurately predicts the filter associated with networks trained using standard gradient descent, our analysis shows that it falls short to explain the behaviour of networks trained using the popular Adam optimizer. The latter achieves a larger change of weights in hidden layers, adapting the non-local filtering function during training. We evaluate our findings via extensive image denoising experiments.</p> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/vLxzxp2boyY" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> </div> </div> </li></ol> </div>]]></content><author><name></name></author><category term="deep-image-prior"/><summary type="html"><![CDATA[Trying to unveil the mystery]]></summary></entry><entry><title type="html">Sketching Single-Photon Data</title><link href="https://tachella.github.io/blog/lidarsketching/" rel="alternate" type="text/html" title="Sketching Single-Photon Data"/><published>2021-10-11T19:22:00+00:00</published><updated>2021-10-11T19:22:00+00:00</updated><id>https://tachella.github.io/blog/lidarsketching</id><content type="html" xml:base="https://tachella.github.io/blog/lidarsketching/"><![CDATA[<p>Single-photon lidar is an emerging ranging technique that can obtain 3D information at kilometre distance with centimetre precision, and has important applications in self-driving cars, forest canopy monitoring, non-line-of-sight imaging and more. This modality consists of contracting a histogram of time-of-arrival of individual photons per pixel. For each object in the line-of-sight of the device there is a peak in the histogram. These peaks are found by a 3D reconstruction algorithm that takes into account the Poisson statistics of the photon-count data, while promoting spatial smoothness in the reconstructed point clouds. In a previous post, I presented an algorithm that can find multiple peaks per pixel in a matter of milliseconds even in challenging very long range scenarios with high background noise. As the algorithm needs to process the histogram data, the <strong>reconstruction time</strong> depends (linearly) on the total number of non-zero bins in the histogram:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/timing_bins-480.webp 480w,/assets/img/timing_bins-800.webp 800w,/assets/img/timing_bins-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/timing_bins.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Execution time of a 3D reconstruction algorithm as a function the number of non-zero bins in the collected time-of-arrival histograms </div> <p>As single-photon lidar arrays get bigger and faster, the number of photons collected per histogram gets bigger, while there is an increased need for faster real-time frame rates. The volume of photon data that needs to be transmitted is ever-increasingly large, generating a <strong>data transfer bottleneck</strong>. Moreover, reconstruction algorithms are required to deal with ever-increasingly large and dense histograms, generating a <strong>computational bottleneck</strong>. So far, most attempts to alleviate these bottlenecks consisted in building coarser histograms. Despite reducing the amount of information to be transferred and processed, this approach sacrifices important depth resolution.</p> <p>In <a class="citation" href="#sheehan2021sketching">(Sheehan et al., 2021)</a>, we propose a sketching method to massively <strong>compress the histograms without any significant loss of information</strong>, removing the data and computational bottlenecks. The technique builds on recent advances in <a href="https://arxiv.org/abs/1706.07180">compressive learning</a>, a theory for compressing distributions. The compressed data consists of a series of \(K\) statistics</p> \[\Phi_k(t) = [\cos(w_k t), \sin(w_kt)]^{T} \quad \text{for} \quad k=1, \dots, K\] <p>where \(t\) denotes the time of arrival. The statistics can be <strong>computed on-the-fly</strong>, i.e. updated with each photon arrival, hence completely by-passing the need to construct a histogram. Below you can see the large difference between reducing the data by coarse binning the histogram and our proposed method:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sheehan2021sketching2-480.webp 480w,/assets/img/sheehan2021sketching2-800.webp 800w,/assets/img/sheehan2021sketching2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/sheehan2021sketching2.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Coarse binning and sketching </div> <p>In <a class="citation" href="#sheehan2021detection">(Sheehan et al., 2021)</a>, we propose detection methods (i.e., deciding whether there is a surface in a given pixel), which only require access to the sketched data and perform similarly to the other detection methods which require access to time-of-arrival histograms.</p> <p>In <a class="citation" href="#tachella2022srt3d">(Tachella et al., 2022)</a>, we introduce a framework for using sketching together with spatially regularised reconstruction method, which can be applied to most existing spatial reconstruction methods (for example the ones in <a href="/blog/3Dreconstruction/">this project</a>, and is able to massively reduce their computational complexity in mid and high photon regimes.</p> <h3 id="related-papers">Related papers</h3> <div class="publications"> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tachella2022srt3d.PNG-480.webp 480w,/assets/img/publication_preview/tachella2022srt3d.PNG-800.webp 800w,/assets/img/publication_preview/tachella2022srt3d.PNG-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/tachella2022srt3d.PNG" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tachella2022srt3d.PNG" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="tachella2022srt3d" class="col-sm-8"> <div class="title">Sketched RT3D: How to reconstruct billions of photons per second</div> <div class="author"> <em>Julian Tachella</em>,&nbsp;Michael P. Sheehan ,&nbsp;and&nbsp;Mike Davies </div> <div class="periodical"> <em>Best Student Paper Award at ICASSP’22</em>, Mar 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2203.00952" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://gitlab.com/Tachella/real-time-sp-lidar" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Single-photon light detection and ranging (lidar) captures depth and intensity information of a 3D scene. Reconstructing a scene from observed photons is a challenging task due to spurious detections associated with background illumination sources. To tackle this problem, there is a plethora of 3D reconstruction algorithms which exploit spatial regularity of natural scenes to provide stable reconstructions. However, most existing algorithms have computational and memory complexity proportional to the number of recorded photons. This complexity hinders their real-time deployment on modern lidar arrays which acquire billions of photons per second. Leveraging a recent lidar sketching framework, we show that it is possible to modify existing reconstruction algorithms such that they only require a small sketch of the photon information. In particular, we propose a sketched version of a recent state-of-the-art algorithm which uses point cloud denoisers to provide spatially regularized reconstructions. A series of experiments performed on real lidar datasets demonstrates a significant reduction of execution time and memory requirements, while achieving the same reconstruction performance than in the full data case.</p> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/mD76r-OuNtc" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sheehan2021sketching-480.webp 480w,/assets/img/publication_preview/sheehan2021sketching-800.webp 800w,/assets/img/publication_preview/sheehan2021sketching-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/sheehan2021sketching.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sheehan2021sketching.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="sheehan2021sketching" class="col-sm-8"> <div class="title">A sketching framework for reduced data transfer in photon counting lidar</div> <div class="author"> Michael P Sheehan ,&nbsp;<em>Julian Tachella</em>,&nbsp;and&nbsp;Mike E Davies </div> <div class="periodical"> <em>IEEE Transactions on Computational Imaging</em>, Mar 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2102.08732" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a href="https://ieeexplore.ieee.org/abstract/document/9541047" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://gitlab.com/Tachella/sketched_lidar" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Single-photon lidar has become a prominent tool for depth imaging in recent years. At the core of the technique, the depth of a target is measured by constructing a histogram of time delays between emitted light pulses and detected photon arrivals. A major data processing bottleneck arises on the device when either the number of photons per pixel is large or the resolution of the time-stamp is fine, as both the space requirement and the complexity of the image reconstruction algorithms scale with these parameters. We solve this limiting bottleneck of existing lidar techniques by sampling the characteristic function of the time of flight (ToF) model to build a compressive statistic, a so-called sketch of the time delay distribution, which is sufficient to infer the spatial distance and intensity of the object. The size of the sketch scales with the degrees of freedom of the ToF model (number of objects) and not, fundamentally, with the number of photons or the time-stamp resolution. Moreover, the sketch is highly amenable for on-chip online processing. We show theoretically that the loss of information for compression is controlled and the mean squared error of the inference quickly converges towards the optimal Cramér-Rao bound (i.e. no loss of information) for modest sketch sizes. The proposed compressed single-photon lidar framework is tested and evaluated on real life datasets of complex scenes where it is shown that a compression rate of up-to 150 is achievable in practice without sacrificing the overall resolution of the reconstructed image.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sheehan2021detection-480.webp 480w,/assets/img/publication_preview/sheehan2021detection-800.webp 800w,/assets/img/publication_preview/sheehan2021detection-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/sheehan2021detection.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sheehan2021detection.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="sheehan2021detection" class="col-sm-8"> <div class="title">Surface Detection for Sketched Single Photon Lidar</div> <div class="author"> Michael P. Sheehan ,&nbsp;<em>Julian Tachella</em>,&nbsp;and&nbsp;Mike E. Davies </div> <div class="periodical"> <em>In 2021 29th European Signal Processing Conference (EUSIPCO)</em> , Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2105.06920" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a href="https://gitlab.com/Tachella/sketched_lidar" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Single-photon lidar devices are able to collect an ever-increasing amount of time-stamped photons in small time periods due to increasingly larger arrays, generating a memory and computational bottleneck on the data processing side. Recently, a sketching technique was introduced to overcome this bottleneck which compresses the amount of information to be stored and processed. The size of the sketch scales with the number of underlying parameters of the time delay distribution and not, fundamentally, with either the number of detected photons or the time-stamp resolution. In this paper, we propose a detection algorithm based solely on a small sketch that determines if there are surfaces or objects in the scene or not. If a surface is detected, the depth and intensity of a single object can be computed in closed-form directly from the sketch. The computational load of the proposed detection algorithm depends solely on the size of the sketch, in contrast to previous algorithms that depend at least linearly in the number of collected photons or histogram bins, paving the way for fast, accurate and memory efficient lidar estimation. Our experiments demonstrate the memory and statistical efficiency of the proposed algorithm both on synthetic and real lidar datasets.</p> </div> </div> </div> </li></ol> </div>]]></content><author><name></name></author><category term="lidar"/><summary type="html"><![CDATA[compressing large-scale lidar data]]></summary></entry><entry><title type="html">3D Reconstruction From Single-Photon Data</title><link href="https://tachella.github.io/blog/3Dreconstruction/" rel="alternate" type="text/html" title="3D Reconstruction From Single-Photon Data"/><published>2020-02-10T00:00:00+00:00</published><updated>2020-02-10T00:00:00+00:00</updated><id>https://tachella.github.io/blog/3Dreconstruction</id><content type="html" xml:base="https://tachella.github.io/blog/3Dreconstruction/"><![CDATA[<p>Single-photon light detection and ranging (lidar) has emerged as a prime candidate technology for depth imaging through challenging environments. This modality relies on constructing, for each pixel, a histogram of time delays between emitted light pulses and detected photon arrivals. The problem of estimating the number of imaged surfaces, their reflectivity and position becomes very challenging in the low-photon regime (which equates to short acquisition times) or relatively high background levels (i.e., strong ambient illumination).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lidar_summary-480.webp 480w,/assets/img/lidar_summary-800.webp 800w,/assets/img/lidar_summary-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/lidar_summary.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Schematic of 3D reconstruction from lidar data </div> <p>In a general setting, a variable number of surfaces can be observed per imaged pixel. The majority of existing methods assume exactly one surface per pixel, simplifying the reconstruction problem so that standard image processing techniques can be easily applied. However, this assumption hinders practical three-dimensional (3D) imaging applications, being restricted to controlled indoor scenarios. Moreover, other existing methods that relax this assumption achieve worse reconstructions, suffering from long execution times and large memory requirements.</p> <p>This project focuses on novel approaches to 3D reconstruction from single-photon lidar data, which are capable of identifying multiple surfaces in each pixel. A first approach to multi-depth consists of detecting in which pixels a target is present. Limiting the number of surfaces per pixel to 0 or 1 can significantly reduce the complexity of the reconstructions algorithms, while still tackling a wide range of practical imaging scenarios. Detection methods can be found in <a class="citation" href="#tachella2019detection1">(Tachella et al., 2019)</a> and <a class="citation" href="#tachella2019detection2">(Tachella et al., 2019)</a>.</p> <p>The models proposed in <a class="citation" href="#tachella2019manipop">(Tachella et al., 2019)</a>, <a class="citation" href="#tachella2019genmanipop">(Tachella et al., 2019)</a>, <a class="citation" href="#tachella2019rt3d">(Tachella et al., 2019)</a> and <a class="citation" href="#tachella2019crt3d">(Tachella et al., 2019)</a> differ from standard image processing tools, being designed to capture correlations of manifold-like structures.</p> <p>Until now, a major limitation has been the significant amount of time required for the analysis of the recorded data. By combining statistical models with highly scalable computational tools from the computer graphics community, we demonstrate 3D reconstruction of complex outdoor scenes with processing times of the order of 20 ms, where the lidar data was acquired in broad daylight from distances up to 320 m <a class="citation" href="#tachella2019rt3d">(Tachella et al., 2019)</a>. This has enabled robust, real-time target reconstruction of complex moving scenes, paving the way for single-photon lidar at video rates for practical 3D imaging applications</p> <p>Multispectral lidar (MSL) systems gather measurements at many spectral bands, making it possible to distinguish distinct materials. The MSL modality consists of constructing one histogram of time delays per wavelength. 3D reconstruction from MSL data imposes an additional challenge as the data to be processed can become prohibitive. A way to overcome this limitation is through the use of compressive strategies on the spatial domain <a class="citation" href="#tachella2019manipop">(Tachella et al., 2019)</a>.</p> <p>A comprehensive survey of 3D reconstruction methods can be found in <a class="citation" href="#rapp2020advances">(Rapp et al., 2020)</a>.</p> <h3 id="related-papers">Related papers</h3> <div class="publications"> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rapp2020advances-480.webp 480w,/assets/img/publication_preview/rapp2020advances-800.webp 800w,/assets/img/publication_preview/rapp2020advances-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/rapp2020advances.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rapp2020advances.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="rapp2020advances" class="col-sm-8"> <div class="title">Advances in single-photon lidar for autonomous vehicles: Working principles, challenges, and recent advances</div> <div class="author"> Joshua Rapp ,&nbsp;<em>Julian Tachella</em>,&nbsp;Yoann Altmann ,&nbsp;Stephen McLaughlin ,&nbsp;and&nbsp;Vivek K Goyal </div> <div class="periodical"> <em>IEEE Signal Processing Magazine</em>, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/9127841" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The safety and success of autonomous vehicles (AVs) depend on their ability to accurately map and respond to their surroundings in real time. One of the most promising recent technologies for depth mapping is single-photon lidar (SPL), which measures the time of flight of individual photons. The long-range capabilities (kilometers), excellent depth resolution (centimeters), and use of low-power (eye-safe) laser sources renders this modality a strong candidate for use in AVs. While presenting unique opportunities, the remarkable sensitivity of single-photon detectors introduces several signal processing challenges. The discrete nature of photon counting and the particular design of the detection devices means the acquired signals cannot be treated as arising in a linear system with additive Gaussian noise. Moreover, the number of useful photon detections may be small despite a large data volume, thus requiring careful modeling and algorithmic design for real-time performance. This article discusses the main working principles of SPL and summarizes recent advances in signal processing techniques for this modality, highlighting promising applications in AVs as well as a number of challenges for vehicular lidar that cannot be solved by better hardware alone.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tachella2019detection-480.webp 480w,/assets/img/publication_preview/tachella2019detection-800.webp 800w,/assets/img/publication_preview/tachella2019detection-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/tachella2019detection.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tachella2019detection.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="tachella2019detection1" class="col-sm-8"> <div class="title">Fast Surface Detection in Single-Photon Lidar Waveforms</div> <div class="author"> <em>Julian Tachella</em>,&nbsp;Yoann Altmann ,&nbsp;Stephen McLaughlin ,&nbsp;and&nbsp;Jean-Yves Tourneret </div> <div class="periodical"> <em>In Proc. 27th Eur. Signal Process. Conf. (EUSIPCO)</em> , Sep 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ieeexplore.ieee.org/document/8903062" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://gitlab.com/Tachella/lidardetection" class="btn btn-sm z-depth-0" role="button">Code</a> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 abbr"> </div> <div id="tachella2019detection2" class="col-sm-8"> <div class="title">On fast object detection using single-photon lidar data</div> <div class="author"> <em>Julian Tachella</em>,&nbsp;Yoann Altmann ,&nbsp;Stephen McLaughlin ,&nbsp;and&nbsp;Jean-Yves Tourneret </div> <div class="periodical"> <em>In Proc. SPIE Wavelets and Sparsity XVIII</em> , Sep 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11138/111380T/On-fast-object-detection-using-single-photon-lidar-data/10.1117/12.2527685.short" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://gitlab.com/Tachella/lidardetection" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Light detection and ranging (Lidar) systems based on single-photon detection can be used to obtain range and reflectivity information from 3D scenes with high range resolution. However, reconstructing the 3D surfaces from the raw single-photon waveforms is challenging, in particular when a limited number of photons is detected and when the ratio of spurious background detection events is large. This paper reviews a set of fast detection algorithms, which can be used to assess the presence of objects/surfaces in each waveform, allowing only the histograms where the imaged surfaces are present to be further processed. The original method we recently proposed is extended here using a multiscale approach to further reduce the computational complexity of the detection process. The proposed methods are compared to state-of-the-art 3D reconstruction methods using synthetic and real single-photon data and the results illustrate their benefits for fast and robust target detection.</p> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tachella2019manipop-480.webp 480w,/assets/img/publication_preview/tachella2019manipop-800.webp 800w,/assets/img/publication_preview/tachella2019manipop-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/tachella2019manipop.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tachella2019manipop.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="tachella2019manipop" class="col-sm-8"> <div class="title">Bayesian 3D Reconstruction of Complex Scenes from Single-Photon Lidar Data</div> <div class="author"> <em>Julian Tachella</em>,&nbsp;Yoann Altmann ,&nbsp;Ximing Ren ,&nbsp;Angus McCarthy ,&nbsp;Gerald Buller ,&nbsp;Steve McLaughlin ,&nbsp;and&nbsp;Jean-Yves Tourneret </div> <div class="periodical"> <em>SIAM Journal on Imaging Sciences</em>, Sep 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1810.11633" class="btn btn-sm z-depth-0" role="button">arXiv</a> <a href="https://hal.archives-ouvertes.fr/hal-02185077/document" class="btn btn-sm z-depth-0" role="button">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://gitlab.com/Tachella/manipop" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Light detection and ranging (Lidar) data can be used to capture the depth and intensity profile of a 3D scene. This modality relies on constructing, for each pixel, a histogram of time delays between emitted light pulses and detected photon arrivals. In a general setting, more than one surface can be observed in a single pixel. The problem of estimating the number of surfaces, their reflectivity and position becomes very challenging in the low-photon regime (which equates to short acquisition times) or relatively high background levels (i.e., strong ambient illumination). This paper presents a new approach to 3D reconstruction using single-photon, single-wavelength Lidar data, which is capable of identifying multiple surfaces in each pixel. Adopting a Bayesian approach, the 3D structure to be recovered is modelled as a marked point process and reversible jump Markov chain Monte Carlo (RJ-MCMC) moves are proposed to sample the posterior distribution of interest. In order to promote spatial correlation between points belonging to the same surface, we propose a prior that combines an area interaction process and a Strauss process. New RJ-MCMC dilation and erosion updates are presented to achieve an efficient exploration of the configuration space. To further reduce the computational load, we adopt a multiresolution approach, processing the data from a coarse to the finest scale. The experiments performed with synthetic and real data show that the algorithm obtains better reconstructions than other recently published optimization algorithms for lower execution times.</p> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/pk0tLCCqnVk" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tachellagenmanipop-480.webp 480w,/assets/img/publication_preview/tachellagenmanipop-800.webp 800w,/assets/img/publication_preview/tachellagenmanipop-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/tachellagenmanipop.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tachellagenmanipop.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="tachella2019genmanipop" class="col-sm-8"> <div class="title">3D Reconstruction Using Single-photon Lidar Data Exploiting the Widths of the Returns</div> <div class="author"> <em>Julian Tachella</em>,&nbsp;Yoann Altmann ,&nbsp;Stephen McLaughlin ,&nbsp;and&nbsp;Jean-Yves Tourneret </div> <div class="periodical"> <em>In Proc. Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)</em> , May 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://gitlab.com/Tachella/generalized-manipop" class="btn btn-sm z-depth-0" role="button">Code</a> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tachella2019rt3d-480.webp 480w,/assets/img/publication_preview/tachella2019rt3d-800.webp 800w,/assets/img/publication_preview/tachella2019rt3d-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/tachella2019rt3d.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tachella2019rt3d.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="tachella2019rt3d" class="col-sm-8"> <div class="title">Real-time 3D reconstruction from single-photon lidar data using plug-and-play point cloud denoisers</div> <div class="author"> <em>Julian Tachella</em>,&nbsp;Yoann Altmann ,&nbsp;Nicolas Mellado ,&nbsp;Rachel Tobin ,&nbsp;Angus McCarthy ,&nbsp;Gerald Buller ,&nbsp;Jean-Yves Tourneret ,&nbsp;and&nbsp;Steve McLaughlin </div> <div class="periodical"> <em>Nature Communications</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.nature.com/articles/s41467-019-12943-7.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a class="abstract btn btn-sm z-depth-0" role="button">Video</a> <a href="https://gitlab.com/Tachella/real-time-single-photon-lidar" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Single-photon lidar has emerged as a prime candidate technology for depth imaging through challenging environments. Until now, a major limitation has been the significant amount of time required for the analysis of the recorded data. Here we show a new computational framework for real-time three-dimensional (3D) scene reconstruction from single-photon data. By combining statistical models with highly scalable computational tools from the computer graphics community, we demonstrate 3D reconstruction of complex outdoor scenes with processing times of the order of 20 ms, where the lidar data was acquired in broad daylight from distances up to 320 metres. The proposed method can handle an unknown number of surfaces in each pixel, allowing for target detection and imaging through cluttered scenes. This enables robust, real-time target reconstruction of complex moving scenes, paving the way for single-photon lidar at video rates for practical 3D imaging applications.</p> </div> <div class="abstract hidden"> <div style="text-align: center;"> <figure> <iframe src="https://youtu.be/PzCcAoypUfM" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> </div> </div> </li> <li><div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tachella2019crt3d-480.webp 480w,/assets/img/publication_preview/tachella2019crt3d-800.webp 800w,/assets/img/publication_preview/tachella2019crt3d-1400.webp 1400w," sizes="200px" type="image/webp"/> <img src="/assets/img/publication_preview/tachella2019crt3d.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tachella2019crt3d.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div id="tachella2019crt3d" class="col-sm-8"> <div class="title">Real-time 3D color imaging with single-photon lidar data</div> <div class="author"> <em>Julian Tachella</em>,&nbsp;Yoann Altmann ,&nbsp;Stephen McLaughlin ,&nbsp;and&nbsp;Jean-Yves Tourneret </div> <div class="periodical"> <em>In Proc. 8th Int. Workshop Comput. Adv. Multi-Sensor Adap. Process. (CAMSAP)</em> , Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://gitlab.com/Tachella/real-time-single-photon-lidar" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Single-photon lidar devices can acquire 3D data at very long range with high precision. Moreover, recent advances in lidar arrays have enabled acquisitions at very high frame rates. However, these devices place a severe bottleneck on the reconstruction algorithms, which have to handle very large volumes of noisy data. Recently, real-time 3D reconstruction of distributed surfaces has been demonstrated obtaining information at one wavelength. Here, we propose a new algorithm that achieves color 3D reconstruction without increasing the execution time nor the acquisition process of the realtime single-wavelength reconstruction system. The algorithm uses a coded aperture that compresses the data by considering a subset of the wavelengths per pixel. The reconstruction algorithm is based on a plug-and-play denoising framework, which benefits from off-the-shelf point cloud and image de-noisers. Experiments using real lidar data show the competitivity of the proposed method.</p> </div> </div> </div> </li></ol> </div>]]></content><author><name></name></author><category term="lidar"/><summary type="html"><![CDATA[Single-photon light detection and ranging (lidar) has emerged as a prime candidate technology for depth imaging through challenging environments. This modality relies on constructing, for each pixel, a histogram of time delays between emitted light pulses and detected photon arrivals. The problem of estimating the number of imaged surfaces, their reflectivity and position becomes very challenging in the low-photon regime (which equates to short acquisition times) or relatively high background levels (i.e., strong ambient illumination).]]></summary></entry></feed>